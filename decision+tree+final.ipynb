{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_data = pd.read_csv('C:/Users/anirban.dey/Desktop/data_2.csv',\n",
    "sep= ',', header= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Dataset Lenght:: \"), len(balance_data)\n",
    "print (\"Dataset Shape:: \"), balance_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Dataset:: \")\n",
    "balance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = balance_data.values[:, 1:5]\n",
    "Y = balance_data.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,\n",
    " max_depth=3, min_samples_leaf=5)\n",
    "clf_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_en = clf_entropy.predict(X_test)\n",
    "y_pred_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy is \"), accuracy_score(y_test,y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def funwithanagrams(text):\n",
    "# Write your code here\n",
    "    \n",
    "    anagram_dict = defaultdict(list)\n",
    "\n",
    "    for word in text:\n",
    "        sorted_word = ''.join(sorted(word))\n",
    "        if sorted_word in anagram_dict:\n",
    "            anagram_dict[sorted_word].append(word)\n",
    "        else:\n",
    "            anagram_dict[sorted_word] = [word]\n",
    "\n",
    "\n",
    "    result = []\n",
    "    seen_anagrams = set()\n",
    "\n",
    "    for key in anagram_dict.values():\n",
    "        if len(key) > 1:\n",
    "            result.append(key[0])\n",
    "            seen_anagrams.add(key[0])\n",
    "\n",
    "            for word in key[1:]:\n",
    "                result.append(word)\n",
    "                \n",
    "        else:\n",
    "            result.append(key[0])\n",
    "            \n",
    "    \n",
    "    final_result = []\n",
    "    \n",
    "    for word in result:\n",
    "        if word not in seen_anagrams:\n",
    "            final_result.append(word)\n",
    "        else:\n",
    "            seen_anagrams.remove(word)\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "text = [\"code\", \"doce\", \"ecod\", \"frame\", \"frame\"]\n",
    "\n",
    "\n",
    "funwithanagrams(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how\n",
      "many\n",
      "eggs\n",
      "are\n",
      "in\n",
      "halfdozen\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "def normalized_words(text):\n",
    "        # Write your code here\n",
    "    stop_words = [\"a\", \"and\", \"be\", \"is\", \"of\", \"the\", \"to\"]\n",
    "\n",
    "    normalized_list = []\n",
    "\n",
    "    # convert to slit of words\n",
    "    text_list = text.split(\" \")\n",
    "\n",
    "    for word in text_list:\n",
    "        updated_word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        if updated_word not in stop_words:\n",
    "            normalized_list.append(updated_word.lower())\n",
    "            print(updated_word)\n",
    "\n",
    "    return len(normalized_list)\n",
    "\n",
    "\n",
    "\n",
    "words_list = \"how many eggs are in and half-dozen, 13!!?\"\n",
    "normalized_words(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ho\n",
      "many\n",
      "eggs\n",
      "are\n",
      "in\n",
      "halfdozen\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def normalize(words):\n",
    "\n",
    "    word_list = []\n",
    "\n",
    "    for word in words:\n",
    "        word_str = word.translate(str.maketrans('', '', string.punctuation))\n",
    "        print(word_str.lower())\n",
    "        word_list.append(word_str.lower())\n",
    "\n",
    "    return len(word_list)\n",
    "\n",
    "\n",
    "words_new = [\"Ho\",\"many\", \"eggs\", \"Are\", \"in\", \"Half-dozen\", \"13\"]\n",
    "normalize(words_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
